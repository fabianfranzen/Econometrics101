{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Econometrics 101\n",
    "What we will learn in this course:\n",
    "Statistics and probability\n",
    "Descriptive Statistics\n",
    "\n",
    "Part 1: The classical linear regression model\n",
    "Classical linear regression modell with one or more regressors\n",
    "Inferens and functional form\n",
    "The problems with multikollinearity, heteroskedacity and autocorrelation\n",
    "\n",
    "Part 2: When assumptions for the classical regression model are not met\n",
    "Multicollinearity, Specification errors (functional forms, omitted variables), heteroskedacity, robust standard errors, autocorrelation\n",
    "Regression with discrete outcome variables, paneldata, instrument variables, time serie analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random variables and discrete/continuous distributions\n",
    "Random variables (RV) and Probability Density Functions (PDF)\n",
    "RVs are outcome from random selections (Discrete or Continuous) and this distribution is often described with a PDF. It is often capitalized.\n",
    "\n",
    "A discrete random variable is one which only takes a countable number of distinct values and can thus be quantified. The density function of a dice roll is the probability to roll a certain dice roll. So if you roll two dice 36 times and receive 1 snake eyes (two 1's) then the probability to roll snake eyes is 2.78%.\n",
    "\n",
    "If random variable X may take k different values, with the probability of X = xi defined to be P(X = xi) = pi. Then the probabilities pi must satisfy the following:\n",
    "\n",
    "1: 0 < pi < 1 for each i\n",
    "2: p1 + p2 + ... + pk = 1\n",
    "\n",
    "so for i = 1,2...n and f(x) = 0 for x =/= xi:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( f(x) \\right) = \\left(P(X = x{_i}) \\right) \n",
    "\\end{equation*}\n",
    "\n",
    "Examples of discrete random variables are Bernoulli distribution, Binomial distribution, Poisson distribution.\n",
    "\n",
    "A continuous random variable takes on an infinite number of possible values. If you define the random variable X to be the height of students in a class the continuous random variable is defined over an interval of values, represented by the area under a curve or integral.\n",
    "\n",
    "The probability of a continuous random variable, the probability distribution function, are the functions that takes on continuous values. The probability of observing any single value is equal to 0 since the number of values may be assumed by the random variable is infinite. An example is what is the probability of a person being 173.22222 cm in a classroom of 30 people vs being 173.22221 or 173.22223? Not high, close to 0. \n",
    "\n",
    "For a random variable X, the probability that X is in the set of outcomes A, P(A), is defined by the area above A and under the curve. The curve, which represents a function p(x), must satisfy the following:\n",
    "\n",
    "1: The curve has no negative values, p(x) > 0 for all x\n",
    "2: The total area under the curve is equal to 1\n",
    "\n",
    "so:\n",
    "\\begin{equation*}\n",
    "\\left( \\sum_{X} \\int_{-\\infty}^\\infty xf(x)dx \\right) = 1\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "A curve meeting these requirements is known as a density curve. Ecamples of continuous probability distributions are normal distributions, exponential distributions, beta distributions etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moment\n",
    "\n",
    "A distribution can be summarized by a couple of features that are called the moments of the distribution. The most common of these is the average (expected value) and the variance. The third moment is skewness and the fourth is kurtosis, these can be thought of as symmetri and height respectively. \n",
    "\n",
    "Expected value (väntevärde på svenska):\n",
    "The expected value is represented as E(X).\n",
    "\n",
    "So for a discrete RV the E(X) can be calculated as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( E(X) \\right) = \\left( \\sum_{X} xf(x) \\right) = \\left( \\sum_{X} xP(x) \\right) = \\left(\\mu \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "and for a continuous RV the E(X) can be calculated as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( E(X) \\right) = \\left( \\sum_{X} \\int_{-\\infty}^\\infty xf(x)dx \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "So following the above and the example of the dice roll in the previous segment the continuous PDF of a dice toss would be:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\left( f(x) \\right) = \\left( 1/9x^2 \\right)\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "\\ 0 < x < 3\n",
    "\\end{equation*}\n",
    "so\n",
    "\\begin{equation*}\n",
    "\\left( E(X) \\right) = \\left( \\int_0^3 (x^2/9)dx \\right) = \\int_0^3 (x^3/9)dx = [x^4/36]_0^3 = 2.25\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "There are three types of data regarding to econometrics.\n",
    "Time series data, observations during a period of time. An example of this is unemployment.\n",
    "Cross section data (Tvärsnittsdata), observations over several subjects at a certain point in time. An example of this is the level of unemployment and education for cities in the UK in november 2015.\n",
    "Pooled data/panel data, observations spanning both room and time, a combination of the above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "A regression analysis seeks to find the link between the dependant variable, Y, and one or several explanatory variables\n",
    "\n",
    "Population regression function (PRF)  is the \"true\" regression for two or more variables gor a given population.\n",
    "\n",
    "Sample regression function (SRF) is the regression of a sample of the population which is used to draw conclusion about the underlying PRF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chow test\n",
    "The chow test is a statistical and econometric test of whether the coefficients in two linear regressions on different data sets are equal and is used on time series analysis to test for the prescence of a structural break or if the independant variables have different impacts on different subgroups of the population.\n",
    "\n",
    "#### Dummy variables\n",
    "A dummy variable (aka an indicator variable) is a numeric variable that represents categorical data, such as gender, race, political affiliation etc. Generally if the dummy variable is 1 it represents that the attribute is present and 0 represents absence.\n",
    "\n",
    "The dummy variable trap is a common mistake where one defines too many variables. If a categorical variable can take on k values it is tempting to define k dummy variables, but you only need k-1 dummy variables otherwise you risk multicollinearity.\n",
    "\n",
    "#### P-value\n",
    "To investigate the p-value is a fast way to reach the conclusion if the parameter is significantly different from zero or not. If the P-value equals or is greater than the specified significance level: H0 is concluded. If the P-value is less than the significance level: H1 is concluded.\n",
    "\n",
    "### t-test and F-test\n",
    "\n",
    "If the test value is larger than the critical value we reject the null hypothesis!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS - ORDINARY LEAST SQUARES\n",
    "\n",
    "#### Gauss Markov\n",
    "\n",
    "The Gauss-Markov theorem states that if your linear regression model satisfies the first six classical assumptions, then ordinary least squares (OLS) regression produces unbiased estimates that have the smallest variance of all possible linear estimators. In other words, when you satisfy these classical assumptions you are obtaining the best coefficient estimates. The Gauss Markov theorem does not state that these are just the best possible estimates for OLS, but the best possible estimates for ANY linear model estimator. \n",
    "\n",
    "If the Gauss Markov Theorem holds that means that the OLS is BLUE: Best Linear Unbiased Estimator!\n",
    "Quick reminder: The notation for the model of a population has Beta (β) which represents the population parameter and Epsilon (ε) represents the random error that the model doesn't explain. The notation for an estimated model has hats (^) over the betas which indicate that these are parameter estimates while (e) reprisents the residuals which are estimates of the random error.\n",
    "\n",
    "#### Unbiased Estimates: Sampling Distributions Centered on the True Population Parameter\n",
    "Does the OLS produce the correct estimate on average? If a model produces estimates which are too high, it shifts to the right from the true beta and there is a positive bias and not correct on average. The true curve would center on the actual value of beta which represents the true population value. In essence if the estimates are too high or low the curve will be shifted off the true beta.\n",
    "\n",
    "#### Minimum Variance: Sampling Distributions are Tight Around the Population Parameter\n",
    "Two curves can both center on the True Beta but one curve can be wider than another IF it has a greater variance. Broader curves indicate that there is a higher probability that the estimates will be further away from the correct value. We want estimates close to beta. Both curves will be correct on average, but we want to follow the narrower curve since it is likelier that they're closer to the correct value than the wider curve. \n",
    "The \"Best\" in BLUE refers to the sampling distribution with the minimum variance.\n",
    "\n",
    "In short you want the OLS estimator to be unbiased (väntevärdesriktig), Effective (less variance is better) and Consistent (asymptotic quality)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss Markov Assumptions\n",
    "When these 7 assumptions are true, OLS produces the best estimates, but if they are not true other estimations might provide better results.\n",
    "\n",
    "Summary: Linear coefficents/error, unbiased error, independant variables uncorrelated with error term, no autocorrelation, no heteroskedasticity, no multicollinearity, normal distribution.\n",
    "\n",
    "\n",
    "1. The Regression model is linear in the coefficients and the error term.\n",
    "In statistics a regression is linear when all the terms in the model are either the constant or a parameter multiplied by an independant value. In the OLS equation the betas are the parameters of the OLS estimates. Epsilon is the random error. The defining character of a linear regression is this form of the parameters rather than model curvature! Linear models can model curvature by including nonlinear variables such as polynomials and trandorming exponential functions. \n",
    "\n",
    "2. The error term has a population mean of zero\n",
    "The error term accounts for the variation in the dependant variable that the independant variables do not explain. For your model to be unbiased, the average value of the error term must equal zero. If the average error is +7 then our model systematically underpredicts the observed values in what is refered to as a bias. If the expected value is not zero the error term is predictable and we should add that information to the regression model itself! We only want NON-PREDICTABLE (random) errors in the error term.\n",
    "\n",
    "3. All independant variables are uncorrelated with the error term\n",
    "If an independant variable is correlated with the error term, we can use the independant variable to predict the error term. This assumption is also referred to as exogeneity. When the error term is correlated with independant variables it is called endogeneity. Violations of this can occur when there is simultaneity between the independant and dependant variables, omitted variable bias, or measurement error in the dependant variables. Violating this assumption biases the coefficient estimate. \n",
    "\n",
    "4. Observations of the error term are uncorrelated with each other (no autocorrelation)\n",
    "One observation of the error term should not predict the next observation. So if the error for one observation is positive and that systematically increases the probability that the error is positive, that is a positive correlation. If the subsequent error is more likely to have the opposite sign, that is a negative correlation. This problem is known as serial correlation and autocorrelation. Like previously stated: If you have the information to predict the error term, that needs to be incorporated into the model!\n",
    "\n",
    "5. The error term has a constant variance (no heteroscedasticity)\n",
    "The variance of errors should be consistent for all observations, the variance does not change for each observation or for a range of observations. This preffered condition is known as homoscedasticity (same scatter). If the variance changes, we refer to that as heteroscedasticity. This can be checked in a residuals vs fitted value plot. The spread of residuals increases as the fitted value increases and thus appears like a cone in heteroskedasticity.\n",
    "\n",
    "6. No independant variable is a perfect linear function of other explanatory variables. (No multicollinearity)\n",
    "Perfect correlation occurs when two variables have a coefficient of +1 or -1. Perfect correlation suggests that two variables are different forms of the same variable. OLS cannot distinguish one variable from the other when they are perfectly correlated. High enough correlation can also cause problems because it reduces the precision of the OLS regression. One method of solving this is removing one of the correlated variables.\n",
    "\n",
    "7. The error term is normally distributed (optional).\n",
    "OLS does not require the error term to be normally distributed to produce unbiased estimates with the minimum variance. However, satisfying this assumption allows you to perform statistical hypothesis testing and generate reliable confidence intervals. This can be determined is to check if residuals follow a normal distribution in a normal probability plot.\n",
    "\n",
    "In a nutshell, your linear model should produce residuals that have a mean of zero, have a constant variance and are not correlated with themselves or other variables. If this is true OLS produces the best possible estimations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret R-Squared in Regression Analysis\n",
    "\n",
    "R-Squared is a goodness-of-fit measure for linear regression models. This statistic indicates the percentage of variance in the dependant variable that the independant variables explain collectively. R-squared measures the strength of the relationship between your model and the dependant variable on a 0-100% scale.\n",
    "\n",
    "Linear regression identifies the equation that produces the smallest difference between all of the observed values and their fitted values. To be precise, linear regression finds the smallest sum of squared residuals that is possible for the dataset (Residuals are the distance between the observed value and the fitted value). The regression model fits the data well if the differences between the observations and the predicted values are small and unbiased.\n",
    "\n",
    "R-squared evaluates the scatter of the data points around the fitted regression line. It is also called the coefficient of determination. Higher R-squared values represent smaller differences between the observed data and the fitted values. R-squared is the percentage of the dependant variable variation that a linear model explains. When a regression model accounts for more of the variance, the data points are closer to the regression line and thus the R-squared will be higher.\n",
    "\n",
    "TSS = ESS + RSS, R^2 = ESS/TSS = 1-(RSS/TSS)\n",
    "ESS = Explained Sum of Squares\n",
    "RSS = Residual Sum of Squares\n",
    "TSS = Total Sum of Squares\n",
    "\n",
    "\n",
    "#### Limitations\n",
    "You cannot use R-squared to determine if the coefficient estimates and predictions are biased, so you should always check a residual plot. R-squared also doesn't evaluate if a regression model provides an adequate fit to your data. A good model can have a low R-squared value and a biased model can have a high R-squared value!\n",
    "\n",
    "Low R-squared values can be perfectly good models for several reasons:\n",
    "Some fields of study have a greater amount of unexplainable variation. In these areas, your r-squared values are bound to be lower. Fields that try to explain human behavior generally have R-squared values less than 50%, because people are harder to predict than physical things. Fortunately, if you have a low R-squared value but the independant values are statistically significant, you can draw important conclusions about the relationships between the variables. Statistically significant coefficients continue to represent the mean change in the dependant variable given a one unit shift in the independant variable. \n",
    "\n",
    "If you need to generate predicitions that are very precise, a low R-squared value can be a show stopper.\n",
    "\n",
    "On the flipside, high R-squared values aren't always great:\n",
    "The R-squared can be very high if the regression line consistently over- and underpredicts the data along the curve, which is bias. An unbiased models has residuals that are randomly scattered around zero. Non-random residual patterns indicate a bad fit despite high R-squared. This typer of specification bias occurs when your linear model is underspecified. In other words, it is missing significant independant variables, polynomial terms, and interaction terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Multicollinearity \n",
    "Multicollinearity occurs when independant variables in a regression model are correlated. This correlation is a problem because independant variables should be INDEPENDANT. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret results.\n",
    "\n",
    "The key goal of regression analysis is to isolate the relationship between each independant variable and the dependant variable. The interpretation of a regression coefficient is that it represents the mean change in the dependant variable of each for each unit change in an independant variable when you hold all the other independant variables constant. However, when independant variables are correlated, a change in one variable will shift the other. The stronger this correlation is, the more difficult to estimate the relationship between each independant variable and the dependant variable independantly because the independat variables tend to change in unison. \n",
    "\n",
    "#### Problems from Multicollinearity\n",
    "Multicollinearity causes two basic problems:\n",
    "1. The coefficient estimates can swing wildly based on which other independant variables are in the model. The coefficients become very sensitive to small changes in the model.\n",
    "2. Multicollinearity reduces the precision of the estimate coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independant variables that are statistically significant. \n",
    "\n",
    "It's not good if a small change in a model can lead to drastically different conclusions, because then you don't feel like you know the actual effect of each variable!\n",
    "\n",
    "#### Do I have to fix multicollinearity?\n",
    "\n",
    "The need to reduce multicollinearity depends on it's severity and you primary goal for your regression model. \n",
    "\n",
    "1. The severity of the problems increases with the degree of the multicollineariy. Therefore, if you have only moderate multicollinearity, you may not need to resolve it.\n",
    "\n",
    "2. Mulcollineariy affects only the specific independant variables that are correlated. Therefore, if multicollinearity is not present for the independant variables that you are particularly interested in, you may not need to resolve it. \n",
    "\n",
    "3. Multicollinearity effects the coefficients and p-values, but it does not influence the predicitions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don't understand the role of each independant variable, you don't need to reduce severe multicollinearity.\n",
    "\n",
    "#### Testing Multicollinearity with Variance Inflation Factors (VIF)\n",
    "\n",
    "The VIF indentifies correlation between independant variables and the strength of that correlation. VIF starts at 1 and has no upper limit. A value of 1 indicates that there is no correlation between this independant variable and any others. VIFs between 1 and 5 suggest that there is a moderate correlation, but not severe enough to warrant corrective measures. VIFs greater than 5 represent critical levels of multicollinearity where the coefficients are poorly estimated, and the p-values are questionable.\n",
    "\n",
    "\n",
    "#### How to deal with multicollinearity\n",
    "\n",
    "Potential solutions (all with potential drawbacks):\n",
    "1. Remove some of the highly correlated independant variables\n",
    "2. Linearly combine the independant variables, such as adding them together\n",
    "3. Perform an analysis designed for highly correlated variables such as partial least squares regression.\n",
    "\n",
    "There are two types of multicollinearity: Data, correlation in the data, and structural, correlation in models such as X and X^2. Centering the variables is also known as standardizing the variables by subtracting the mean and then using the centered variables in your model. \n",
    "\n",
    "#### Summary\n",
    "\n",
    "Multicollineariy consquences: \n",
    "1. This estimates are still BLUE (OLS)\n",
    "2. Beta coeffcients can be determined, but the variance is high which results in high standard errors\n",
    "3. The estimates will be unprecise which results in high confidence intervalls and low t-values.\n",
    "4. Sensitive to small changes in the model or data\n",
    "5. The coefficients can be statistically insignificant while also having a high goodness-of-fit\n",
    "\n",
    "Indications of multicollinearity:\n",
    "1. High r-squared with low significant estimates\n",
    "2. High correlation between independant variables\n",
    "3. Can be checked through VIF (1, 1-5, >5)\n",
    "\n",
    "How to fix it:\n",
    "1. DO nothing\n",
    "2. Remove variables\n",
    "3. Combine data\n",
    "4. Gather new data\n",
    "5. Standardize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heteroskedasticity\n",
    "\n",
    "Heteroskedasticity means unequal scatter. In regression analysis HS is in the context if the residual or error term. Specifically, HS is a systematic change in the spread of the residuals over the range of measured values. HS is a problem because OLS assumes that all residuals are drawn from a population that has a constant variance (homoskedasticity).\n",
    "\n",
    "#### Identifying HS\n",
    "\n",
    "You want plots to display random residuals (no patterns) that are uncorrelated and uniform. If you see patterns in the residuals you have a problem. Heteroskedasticity produces a distinctive fan or cone shape in residual plots. To check for this you need to assess the residuals by fitted plots specifically. The telltale pattern for heteroskedasticity is that as the fitted values increases, the variance of the residuals also increases.\n",
    "\n",
    "#### What causes HS\n",
    "\n",
    "HS occurs more often in datasets with a large range between the largest and the smallest observed values. A common reason for it's existance is that the error variance changes proportionally with a factor. This factor might be a variable in the model. Cross-sectional studies often have a very small and large value range and are more likely to have heteroskedasticity: A cross-sectional study of income can have a large range that extends from poverty to billionaires.\n",
    "A time-series model can have HS if the dependant variable changes significantly from the beginning to the end of the series: If we model the sales of DVD players from their first sale in 2000 to the present, the number of units sold will be vastly different. Also, if you model time-series data and measurement error changes over time, heterskedasticity can be present because regression analysis includes measurement error in the error term, so if measurement error decreases over time due to better methods being introduced you expect the error variance to diminish over time aswell.\n",
    "\n",
    "Example of classic heteroskedasticity: If you model household consumption based on income, you'll find that the variability in consumption increases as income increases. Lower income households are less variable in absolute terms because they need to focus on necessities and there is less room for different spending habits. Higher income households can purchase a wide variety of luxury items, or not, which results in a broader spread of spending habits.\n",
    "\n",
    "#### Pure vs impure HS\n",
    "\n",
    "* Pure HS refers to cases where you specify the correct model and yet you observe non-constant variance in the residual plots.\n",
    "* Impure HS refers to cases where you incorrectly specify the model, and that causes the non-constant variance. When you leave an important variable out of a model, the omitted effect is absorbed by the error term. If the effect of the omitted variable varies throughout the observed range of data, it can produce the telltale signs of heteroskedasticiy in the residual plots.\n",
    "\n",
    "It is important to determine wheter you have pure or impure HS because the solutions are different. If you have impure you need to identify the important variables that have been left out of the model and refit the model with those variables. Often the key is to understand and identify the proportional factor that is associated with the changing variance.\n",
    "\n",
    "#### Problems from HS\n",
    "\n",
    "There are two big reasons you want to fix HS:\n",
    "1. HS does not cause bias in the coefficient estimates, but it does make them less precise. Lower precision increases the likelihood that the coefficient estimates are further from the correct population value.\n",
    "2. Heterskedasticity tends to produce p-values that are smaller than they should be. This effect occurs because heteroskedasticiy increases the variance of the coefficient estimates but the OLS procedure does not detect this increase. Consequently, OLS calculates the t-values and F-values using underestimated amount of variance. This problem can lead you to conclude that a model term is statistically significant when it is not significant.\n",
    "\n",
    "#### How to fix HS\n",
    "First off you should identify the source of the non-constant variance to resolve the problem. A good place to start is a variable that has a large range. \n",
    "\n",
    "1. If your cross-sectional model (i.e. number of automobile accidents by the population of towns and cities) includes large differences between the size of the observations, you can find different ways to specify the model that reduces the impact of the size differential. To do this, change the model from using the raw measure to using the rates and per capita values. Of course, this model answers a slightly different question and you'll need to determine wheter this approach is suitable for both your data and what you need to learn. This is a valid method because it requires very little tinkering with the original data and you just adjust the specific variables that need to be changed in a manner that often makes sense.\n",
    "\n",
    "2. Weighted regression is a method that assigns each data point a weight based on the variance of its fitted value. The idea is to give small weights to observations associated with higher variances to shrink their squared residuals. Weighted regression minimizes the sum of the weighted squared residuals. When you use the correct weights, the heteroskedasticity is replaced by homoskedasticiy. This method requires a lot of manipulation of the data. \n",
    "\n",
    "3. Transform the dependant variable, although this should be a last resort because it involves the most data manipulation. It also makes interpreting the results very difficult because the units of your data are gone. The idea is that you transform your data into different values that produce good looking residuals. \n",
    "\n",
    "#### Summary\n",
    "What is HS: Variance in the residual is not constant.\n",
    "\n",
    "Reasons for HS: Large range in values, better methods for data gathering, model specification, outliers, faulty transformation of data.\n",
    "\n",
    "Consequences:\n",
    "OLS is unbiased and consistent but might not be effective.\n",
    "Standard deviation and variation is biased.\n",
    "t-,f-,chi- statistics no longer follow their distribution\n",
    "\n",
    "## Methods to identify HS\n",
    "\n",
    "#### Goldfeldt-Quandt Test\n",
    "#### Breush Pagan Test\n",
    "#### WLS\n",
    "#### FGLS\n",
    "#### Robust Standard errors\n",
    "\n",
    "Robust standard errors is a technique to obtain unbiased standard errors in OLS coefficients under heteroskedasticity. Typical terms for \"Robust standard errors\" (since this can go under many same names) is White's standard errors and Huber white standard errors. Robust standard errors are usually larger than conventional standard errors. The robus standard error requires a large sample size, if the sample size is too small then the robust t-statistica doesn't follow the t-distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "\n",
    "Autocorrelation is where error terms in a time series transfer from one period to another. In other words, the error for one time period is correlated with the error in subsequent time periods. For example, an underestimate for one's quarters profit can result in an underestimate of profits for subsequent quarters. \n",
    "\n",
    "The most basic form of autocorrelation is the first order autocorrelation and is specified as Ut = p*U(t-1) + Vt where U refers to the error term of the population regression function. As can be seen the error term at period t is a function of the previous time period t-1. The last term V is a so called white noice error term and is a normally distributed random error. This first order autoregression is denoted AR(1) and is the most common type of autocorrelation.\n",
    "\n",
    "Pure Autocorrelation is when the error in one period is correlated with the errors in other periods. This model is assumed to be correctly specified.\n",
    "\n",
    "Impure autocorrelation is caused by a specification error such as an omitted variable or ignoring non linearities.\n",
    "\n",
    "#### Consequences of AK\n",
    "\n",
    "* Inefficient OLS Estimates and any forecast based on those estimates. An efficient estimator gives you the most information about a sample; inefficient estimators can perform well, but require much larger sample sizes to do so.\n",
    "* Exagerrated goodness of fit (for a time series with positive serial correlation and an independant variable that grows over time).\n",
    "* Standard errors that are too small (for a time series with positive serial correlation and an independant variable that grows over time).\n",
    "* T-statistics that are too large.\n",
    "* False positives for significant regression coefficients. In other words, a regression coefficient appears to be statistically significant when it is not. \n",
    "\n",
    "#### Reasons for AK\n",
    "\n",
    "* Faulty modelspecification (left out variables, wrong functional form, lagging independat variables)\n",
    "* Manipulation or transformation of data\n",
    "* Systematic error in measurement\n",
    "* Non-stationary series.\n",
    "\n",
    "#### Types of autocorrelation\n",
    "The most common form of AK is first-order serial correlation, which can be either positive or negative.\n",
    "* Positive serial correlation is where a positive error in one period carries over into a positive error for the following period.\n",
    "* Negative serial correlation is where a negative error in one period carries over into a negative error for the following period\n",
    "\n",
    "Second-order serial correlation is where an error affects data two time periods later. This can happen when your data has seasonality. Orders higher than second-order do happen, but they are rare.\n",
    "\n",
    "\n",
    "#### Testing for AK\n",
    "\n",
    "* Plot of residuals, looking for clusters of residuals on one side of the zero line.\n",
    "* A durbin watson test\n",
    "* A lagrange multiplier test\n",
    "* Breusch Godfrey test\n",
    "* A correlogram. A pattern in the results is an indication of autocorrelation. Any values above zero should be looked at with suspicion.\n",
    "\n",
    "The Durbin Watson test looks for the AR(1) process. H0: No AR(1), H1: AR(1) exists. The test assumes that the errors are normally distributed with a mean of 0 and are stationary. The durbin watson test reports a statistic with a value from 0 to 4 where \n",
    "\n",
    "* 2 is no AK\n",
    "* 0 to <2 is a positive AK (common in time series)\n",
    "* >2 to 4 is a negative AK (less common in time series)\n",
    "\n",
    "A rule of thumb is that test values between 1.5 and 2.5 are normal, and outside of this range should be of concern.\n",
    "\n",
    "The Durbin Watson is restricted to detecting AR(1), the Breusch Godfrey (BG) Test can detect autocorrelation up to any predesignated order p and supports a broader class of regressors. It uses the LM (Lagrange Multiplier) Statistic.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrumental Variable Regression\n",
    "\n",
    "An instrumental variable (IV) is a third variable, Z, used in regression analysis when you have endogenous variables - variables that are influenced by other variables in the model. In other words, you use it to account for unexpected behavior between variables. Using an instrumental variable to identify the hidden (unobserved) correlation allows you to see the true correlation between the explanatory variable and response variable, Y.\n",
    "\n",
    "Z is correlated with the explanatory variable, X, and uncorrelated with the error term, ε, in the equation Y = Xβ + ε.\n",
    "\n",
    "#### What is instrumental variable regression?\n",
    "Instrumental Variables regression (IV) basically splits your explanatory variable into two parts: one part that could be correlated with ε and one part that probably isn't. By isolating the part with no correlation, it's possible to estimate β in the regression equation:\n",
    "Yi = β0 + β1Xi + εi.\n",
    "\n",
    "This type of regression can control for threats to internal validity:\n",
    "* Confounding variables\n",
    "* Measurement error\n",
    "* Omitted variable bias\n",
    "* Simultaneity\n",
    "* Reverse Causality\n",
    "\n",
    "In essence, IV is used when your variables are related in some way; if you have some type of correlation going on between variables (e.g. bidirectional correlation), then you can't use the more common methods like ordinary least squares, because one requirement of those methods is that variables are not correlated.\n",
    "\n",
    "\n",
    "#### Finding instrumental variables\n",
    "IV regression isn't an easy fix for confounding (extra variable that you didn't account for) or other issues; in real life, instrumental variables can be difficult to find and in fact, may not exist at all. You cannot use the actual data to find IVs (you cant perform a regression to identify any) - you must rely on your knowledge about the model's structure and the theory behind your experiment (like economic theory). When looking for IVs, keep in mind that Z should be:\n",
    "\n",
    "* Exogenous - not affected by other variables in the system. This can't be directly tested, you have to use you knowledge of the system to determine if your system has exogenous variables or not.\n",
    "* Correlated with X, an endogenous variable. A very significant correlation is called a strong first stage. Weak correlations can lead to misleading estimates for parameters and standard errors.\n",
    "\n",
    "You can use causal graphs to outline your model structure and identify possible IV's. \n",
    "\n",
    "## Confounding variables\n",
    "A confounding variable is an \"extra\" variable that you didn't account for. They can ruin an experiment and give you useless results. They can suggest that there is correlation when in fact there isn't. They can introduce bias. That's why it's important to know what one is, and how to avoid getting them into your experiment in the first place.\n",
    "\n",
    "In an experiment, the independant variable (X) typically has an effect on your dependant variable (Y). For example, if you are researching whether lack of excercize leads to weight gain, lack of excersize is your independant variable and weight gain is your dependant variable. Confounding variables are any other variable that also has an effect on your dependant variable. They are like extra independant variables that are having a hidden effect on your dependant variables. Confounding variables can cause two major problems:\n",
    "* Increase variance\n",
    "* Increase bias\n",
    "\n",
    "Example: If you test 100 men and 100 women and you find that lack of excersize leads to weight gain. One problem is that the experiment doesn't contain control variables like the use of placebos or random assignment to groups, so you can't for sure say lack of excersize leads to weight gain. One confounding variable is how much people eat. It's also possible that men eat more than women, so sex is a confounding variable. Nothing is mentioned about starting weight, occupation or age either. A poor study like this would lead to bias.\n",
    "\n",
    "Technically confounding isn't at true bias, because usually bias is a result of errors in data collection or measurement. However, one definition of bias is \"the tendency of a statistic to overestimate or underestimate a parameter\", so in this sense, confounding is a type of bias. \n",
    "\n",
    "Confounding bias is the result of having confounding variables in your model. It has a direction, depending on if it over- or underestimates the effects of your model. \n",
    "\n",
    "* Positive confounding is when the observed association is biased away from the null, it overstimates the effect.\n",
    "* Negative confounding is when the observed association is biased toward the null, it underestimates the effect.\n",
    "\n",
    "#### How to reduce confounding variables\n",
    "Make sure you identify all of the possible confounding variables in your study. Make a list of everything you can think of and one by one, consider whether those listed items might influence the outcome of your study. Usually, someone has done a similar study before you. So check the academic databases for ideas about what to include on your list. Once you have figured out the variables, use one of the following techniques to reduce the effect of those confounding variables:\n",
    "\n",
    "1. Bias can be eliminated with random samples\n",
    "2. Introduce control variables to control for confounding variables. You can control age for instance by only measuring 30 year olds.\n",
    "3. Within subject designs test the same subjects each time. Anything could happen to the test subject in the \"between\" period so this period so this doesn't make for perfect immunity from confounding variables.\n",
    "4. Counterbalancing can be used if you have paired designs. In counterbalancing, half of the group is measured under condition 1 and half is measured under condition 2.\n",
    "\n",
    "\n",
    "## Internal validity\n",
    "\n",
    "Internal validity is a way to measure if research is sound (i.e. was the research done right?). It is related to how many confounding variables you have in your experiment. If you run an experiment and avoid confounding variables, your internal validity is high; the more confounding variables you have, the lower the internal validity. In a perfect world, your experiment has a high internal validity. This would allow you to have high confidence that the results of your experiments are caused by only one independant variable.\n",
    "\n",
    "If you fail to use random sampling or control variables at all, your risk of confounding is extremely high. Therefore your internal validity would be very low.\n",
    "\n",
    "#### Things that can affect validity\n",
    "* Regression to the mean - this means the subjects in the experiment with extreme scores will tend to move toward the average.\n",
    "* Pre-testing subjects - this may have unexpected consequences as it may be impossible to tell how the pre-test and during-tests interact. If \"logical reasoning\" is your dependant variable, participants may get clues from pre-test\n",
    "* Changing the instruments during the study\n",
    "* Participants dropping out of the study. This is usually a bigger threat for experimental designs with more than one group.\n",
    "* Failure to complete protocols\n",
    "* Something unexpected changes during the experiment, affecting the dependant variable.\n",
    "\n",
    "## External validity\n",
    "\n",
    "External validity helps to answer the question: can the research be applied to the \"real world\"? If your research is applicable to other experiments, settings, people, and times, the external validity is high. If the research can not be replicated in other situations, external validity is low. It's important to know that your research is effective (internal validity) and that it is effective in other situations. \n",
    "\n",
    "Historically, researchers have focused on internal validity. The scientific rigor of randomized, controlled experiments was often thought to be more important than the generalizability of results. More recently, researchers have been aiming for research that is more generalizable outside the lab. However this isn't as easy as it seems. External validity is one of the most difficult types of validity to achieve. One reason for this is that steps to make external validity high often results in lowering internal validity. Another reason is the multitudes of hidden and confounding variables that can affect your experimental outcome.\n",
    "\n",
    "#### Population and Ecological validity\n",
    "\n",
    "* Population validity answers the question: how well can the research on a sample be generalized to the population as a whole?\n",
    "* Ecological validity answers the question: Are your study results generalizable across different settings?\n",
    "\n",
    "#### Threats to external validity\n",
    "\n",
    "Threats to external validity compromise your confidence in stating that your study results are applicable to other situations. They are explanations of how you  might be wrong in making generalizations. For example, your conclusion might be incorrect, the changes in the dependant variable may not be due to changes in the independant variable, and variation in the dependant variable might be due to other causes. For example, exogenous variables may be competing with the independant variable to explain the study outcome. \n",
    "\n",
    "* Is your sample selected randomly? If not, it may be open to selection bias.\n",
    "* Have you included pretest? In some experiments, pretests influence the outcome. A pretest might clue the subjects in about the ways they are expected to answer or behave.\n",
    "* Are your participants taking multiple versions of the same test? If so, the practice effect might influence your results.\n",
    "* Is your sample composed of a homogenous population, like all low achievers or high achievers? If so, your results won't be generalizable to the average person.\n",
    "* Are the results of the study tainted by the Hawethorne effect? Your study participants might be behaving differently because they know they are in an experimental study.\n",
    "\n",
    "## Extraneous variables\n",
    "Extraneous variables are any variables not intentionally being studied in you experiment or test. In an experiment you are looking to see if one variable has an effect on another variable. In a perfect world you would run an experiment, check the results and be done. In the real world though, other variables (perhaps ones that never crossed your mind) might influence the outcome of an experiment - these are called extraneous variables.\n",
    "\n",
    "An example is you want to see if online learning increases the understanding of the subject. One group uses online knowledge base to study and the other uses a traditional text. Extraneous variables could include prior knowledge of statistics, support from home, socio-economic income, or temperature of the testing room.\n",
    "\n",
    "There are several types of extraneous variables:\n",
    "* Demand characteristics: environmental clues which tell the participant how to behave, like features in the surrounding or researcher's non-verbal behavior.\n",
    "* Experimenter / Investigator effect: Where the researcher unintentionally affects the outcome by giving clues to the participants about how they should behave.\n",
    "* Participant variables, like prior knowledge, health status or any other individual characteristic that could affect outcome.\n",
    "* Situational variables, like noise, lightning or temperature in the environment.\n",
    "\n",
    "One type of extraneous variable is the confounding variable!\n",
    "One way to control extraneous variables is through random sampling. While this does not eliminate extraneous variables, it ensures it is equal between all groups. If random sampling is not used, extraneous variables become a lot bigger of a concern.\n",
    "\n",
    "\n",
    "## Two-Stage Least Squares (2SLS)\n",
    "The two-stage least squares regression analysis is a statistical technique that is used in the analysis of structural equations and is an extension of the OLS method. It is used when the dependant variable's error terms are correlated with the independant variables. Additionaly, it is useful when there are feedback loops in the model. In structural equations modeling, we use maximum likelihood method to estimate the path coefficient\n",
    "\n",
    "2SLS includes four types of variables:\n",
    "\n",
    "1. Dependant variable - Variable that is to be regressed on the exogenous or endogenous variables\n",
    "2. Exogenous variable - These independant variables are included both in the first and second stage regression models. They are not correlated with the random error values in the second stage regression.\n",
    "3. Endogenous variable - Each endogenous variable becomes the dependant in the first stage regression equation. Each is regressed on all exogenous and instrument variables. The predicted values from these regressions replace the original values of the endogenous variables in the second stage regression model.\n",
    "4. Instrument variables - Each endogenous variable becomes the dependant variable in the first stage regression equation. Each is regressed on all exogenous and instrument variables. The predicted values from these regressions replace the original values of the endogenous variables in the second stage regression model.\n",
    "\n",
    "Problematic causal variable: The dependant or endogenous variable whose error term is correlated with the other dependant variable error term. A problematic causal variable is replaced with the substitute variable in the first stage of the analysis.\n",
    "\n",
    "Instruments: An instrument variable is used to create a new variable by replacing the problematic variable.\n",
    "\n",
    "Stages: In ordinary least squares method, there is a basic assumption that the value of the error term is dependant of predictor variables. When this assumption is broken, this technique helps us solve the problem. This analysis assumes that there is a secondary predictor that is correlated to the problematic predictor but not with the error term. Given the existence of the instrument variable, the following two methods are used:\n",
    "\n",
    "* In the first stage, a new variable is created using the instrument variable\n",
    "* In the second stage, the model-estimated values from stage one are then used in place of the actual values of the problematic predictors to compute an OLS model for the response of interest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simultaneous equation models\n",
    "\n",
    "## The structural and reduced form equations\n",
    "\n",
    "If you have three equations that describe the economy:\n",
    "\n",
    "C = a0 + at*Yt + U\n",
    "I = B0 + B1*R + U\n",
    "Y = C + I + G\n",
    "\n",
    "then this is called structural equations. This model has three endogenous variables C, I, Y (consumption, investment, income) and two exogenous variables R and G (function of interest rate and government spending). The coefficients of the structural equations represent the direct effect of change in one of the explantory variables. So if we increase B1 (the marginal propensity to invest as a result from a change in interest rate). This represents the direct effect of a change in interest rate on the net-investment.\n",
    "\n",
    "Assume that we increase the interest rate. This will have a direct effect on the investments in this model, which in a second step via the equilibrium condition will have an effect on the income. The income in term will change the consumption level and since income is endogenous it will have an effect on U since they are correlated. The initial change in interest rate will affect the components in the system until the effect reaches its equilibrium function.\n",
    "\n",
    "The reduced form of a model is the one which the endogenous variables are expressed as functions of the exogenous variables (and perhaps lagged values of endogenous variables). Very roughly, reduced form estimates do not give you the structural, primitive policy invariant behavioral parameters that you (sometimes) care about, such as parameters of an agents utility function or the slopes demand and supply curves.\n",
    "\n",
    "With RFE, you only get functions of those parameters. For some purposes, that can be enough, which is why some people want to see them. For example, you can frequently get the sign of the relationship from RF estimates, but not the magnitude. Once in a blue moon, you can use algebra to solve for structural parameters from the RFE. Finally, it can also the case that some people will not believe the assumptions needed to estimate the structural parameters.\n",
    "\n",
    "## Identification\n",
    "\n",
    "In order to be able to identify the structural equation coefficients they need to be identified.\n",
    "\n",
    "Consider the following two equation system:\n",
    "\n",
    "Q = A0 + AXP + A2 Xx + U (Supply) \n",
    "\n",
    "Q = B0 + Bf + U2 (Demand)\n",
    "\n",
    "This system has 2 endogenous variables (P and Q) and one Exogenous variable (X1). These two equations represent a demand and supply system for a given market. The question is if any of these two equations are identified. To identify the equations follow the following rules:\n",
    "\n",
    "Define the following variables:\n",
    "\n",
    "M = The number of endogenous variables in the model\n",
    "\n",
    "K = The number of variables (endogenous AND exogenous) in the model excluded from the equation under consideration\n",
    "\n",
    "1. If k = m - 1 then the eq is exactly identified\n",
    "2. If k > m - 1 then the eq is over identified\n",
    "3. If k < m - 1 then the eq is under identified\n",
    "\n",
    "When checking the order condition you have to do it for each eq in the system.\n",
    "\n",
    "So for the following system:\n",
    "\n",
    "Y1 = a0 + a1Y2 0 a2X1 + U1\n",
    "Y2 = B0 + B1Y1 + B2x2 0 U2\n",
    "\n",
    "in the first eq we have M-1 = 1 and K=1 since X2 is excluded from the eq. Since M-1 = K the first eq is exactly identified. this should then be done in a similar manner for the second eq."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for discrete choices\n",
    "* Binary models - The dependant variable has 2 possible values (0 or 1) and is thus a dummy variable. Most often you then use LPM, Logit or Probit models. These are called probability models.\n",
    "* Multinominal models - The dependant variable takes a discrete value (1,2,3,...,n) but can take on more than 2 values. This can be ordinal (ranked order without inherent meaning between values) or Nominal (no inherent order).\n",
    "\n",
    "\n",
    "## The linear probability model\n",
    "\n",
    "A linear probablity model (LPM) is a regression model where the outcome variable is binary and one or more explanatory variables are used to predict outcome. Explanatory variables can themselves be binary, or continuous. It is a simple model to estimate and understand and is good for estimating marginal effects. The coefficients are also easy to interpret.\n",
    "\n",
    "In most linear probability models, R-squared has no meaningful intepretation since the regression line can never fit the data perfectly if the dependant variable is binary and the regressors are continuous. This can be seen in the application  below. It is essential to use robust standard errors since the error term in a LPM is always heteroskedastic. This is because the error term does not follow a normal distribution.\n",
    "\n",
    "\n",
    "If you want to model the probability of a applicants mortgage application being denied or accepted you would model it like:\n",
    "\n",
    "deny = beta0 + beta1 x P/I ratio + error term.\n",
    "\n",
    "This means how much does the \"deny\" change in % per each % change in the payment-to-income ratio\n",
    "\n",
    "If the model looks like this instead:\n",
    "\n",
    "deny = beta0 + beta1 x P/I ratio + beta2 x blac + error term.\n",
    "\n",
    "This means how much does the \"deny\" change in % per each % change in the payment-to-income ratio AND if the applicant is black (1) or not (0).\n",
    "\n",
    "The biggest problem with the LPM model is that it is not always realistic for variables to grow linearly. This does not restrict p to lie between 0 and 1\n",
    "\n",
    "Logistic models can be derived in several ways, which makes things confusing. One way is to think about imposing a restriction on the functional form so that probabilities are bounded between 0 and 1 regardless for all values of X.\n",
    "\n",
    "\n",
    "## LPM vs Logit\n",
    "\n",
    "The linear model assumes that the probability p is a linear function of the regressors, while the logistics model assumes that the natural log of the odds p/(1-p) is a linear function of the regressors.\n",
    "\n",
    "p = a0 + a1X1 + a2X2 + … + akXk  (linear)\n",
    "\n",
    "ln[p/(1-p)] = b0 + b1X1 + b2X2 + … + bkXk  (logistic)\n",
    "\n",
    "The major advantadge of the linear model is its interpretability. In the linear model, if a1 is 0.05 that means that one unit increase in X1 is associated with a 5 percentage point increase in the probability that Y is 1. Just about everyone has some understanding of what it would mean to increase by 5 percentage points their probability of something.\n",
    "\n",
    "The logistic model is less interpretable. In the logistic model, if b1 is 0.05, that means that a one-unit increase in X1 is associated with a 0.05 increase in the log odds that Y is 1. This is not intuitive.\n",
    "\n",
    "Because the log odds are so hard to interpret, it is common to report logisitic regression results as odds ratios. To do this we exponentiate both sides of the logistic regression equation and obtain a new equation that looks like this:\n",
    "\n",
    "p/(1-p) = d0 × (d1)^X1 × (d2)^X2 × … × (dk)^Xk\n",
    "\n",
    "Odds ratios seem like they should be intuitive. If d1 = 2, for example, that means that a one unit increase in X1 doubles the odds that Y is 1. That sounds like something we understand. But we dont! We think we understand odds because in everyday speach we use the word \"odds\" in a vague and informal way as a synonym to \"chance\", \"risk\", \"probability\" and \"likelihood\". The word odds has a very specific meaning and so does the odds ratio:\n",
    "\n",
    "p(1-p)\n",
    "\n",
    "Odds ratios are not intuitive! If the probability of something is 40% and you want to double the odds, the probability after is 57% (not 80%!)! If p is close to 0 then doubling the odds is approx the same as doubling p, if p is closer to 1, then doubling the odds is approx the same as halving 1-p. But if p is in the middle then there really is no intuition.\n",
    "\n",
    "The probability p is defined by the function p = e^z / (1+e^z) = 1/1(1+e^-z) if z = beta1 + beta2*x\n",
    "\n",
    "The logit is defined as z = ln(p/1-p)\n",
    "\n",
    "The marginaleffekt = P(1-p)*beta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity\n",
    "\n",
    "A time series has stationarity if a shift in time doesn't cause a change in the shape of the distribution. Basic properties of the distribution like the mean, variance or covariance are constant over time. So a non-stationarity is for instance a graph which shows seasonality over a time period of a year, while stationarity doesnt show any trend at all.\n",
    "\n",
    "Most forecasting methods assume that distribution has stationarity. For example, autocovariance and autocorrelations rely on the assumption of stationarity. An absence of stationarity can cause unexpected or bizarre behaviors, like t-ratios not following t-distribution or high r-squared values assigned to variables that arent correlated at all.\n",
    "\n",
    "However, in real life most data sets aren't stationary. So if you have a data set you are going to have to make it stationary to get any useful predictions from it. This can be done through transformation of the data set (differencing, curve fitting, logaritm or square root). Some models, like seasonality models, cant be transformed and should be broken into smaller pieces.\n",
    "\n",
    "#### Types of stationarity\n",
    "* Strict stationarity means that the joint distribution of any moment of any degree (expected value, variances, third order and higher moments) within the process is never dependant on time. This definition is in practice too strict to be used in a real-life model.\n",
    "* First-order stationarity series have means that never change over time. Any other statistics (like variance) can change.\n",
    "* Second-order stationarity (also called weak stationarity) time series have a constant mean, variance and an autocovariance that doesn't change with time. Other statistics in the system are free to change over time. This constrained version of strict stationarity is very common.\n",
    "* Trend stationarity models fluctuate around a deterministic trend (the series mean). These deterministic trends can be linear or quadratic, but the amplitude (height of one oscillation) of the fluctuations neither increases nor decreases across the series.\n",
    "* Difference-stationary models are models that need one or more differencings to become stationary.\n",
    "\n",
    "It can be difficult to tell if a model is stationary or not. Unlike obvious examples showing seasonality, you can't usually tell by looking at a graph. If you aren't sure about stationarity of a model, a hypothesis test, can help. This includes:\n",
    "\n",
    "* Unit root tests (like Augmented Dickey-Fuller test)\n",
    "* A KPSS test (run as complement to the unit root test)\n",
    "* A run sequence plot\n",
    "* The Priestley-Subba Rao (PSR) Test or Wavelet-Based Test, which are less common tests based on spectrum analysis.\n",
    "\n",
    "#### Augmented Dickey Fuller test\n",
    "The augmented dickey fuller test is a unit root test for stationarity. Unit roots cause unpredictable results in your time series analysis. The ADF test can be used with serial correlation. It handles more complex models than the dickey fuller test, but should be used with caution because it has a relatively high type 1 error rate (wrongly rejecting the null hypothesis).\n",
    "\n",
    "H0: There is a unit root\n",
    "H1: The time series is stationary\n",
    "\n",
    "Before running your ADF test you need to choose a regression based on if you want or dont want a constant and trend in the regression. After this you need to choose a lag length. The lag length should be chosen so that residuals aren't serially correlated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the functional form\n",
    "\n",
    "Level-Level Regression: y = beta0 +  beta1*X + error\n",
    "Dependant = y, independant = x\n",
    "Interpretation of beta: \n",
    "Δy=β1Δx \n",
    "“If you change x by one, we’d expect y to change by β1\"\n",
    "\n",
    "Log-Level Regression: ln(y) = beta0 +  beta1*X + error\n",
    "Dependant = ln(y), independant = x\n",
    "%Δy=100⋅β1⋅Δx \n",
    "“if we change x by 1 (unit), we’d expect our y variable to change by 100⋅β1 percent”\n",
    "\n",
    "Level-Log Regression: y = beta0 +  beta1*ln(X) + error\n",
    "Dependant = y, independant = ln(x)\n",
    "Δy=(β1/100)%Δx  \n",
    "\"If we increase x by one percent, we expect y to increase by (β1/100) units of y.\"\n",
    "\n",
    "Log-log regression: ln(y) = beta0 + beta1*ln(x) + error\n",
    "Dependant = ln(y), independant = ln(x)\n",
    "%Δy=β1%Δx \n",
    "“if we change x by one percent, we’d expect y to change by β1 percent”\n",
    "\n",
    "### Ramsey RESET Test\n",
    "The Ramsey RESET (regression specific error test) has proven to be useful in to detect general functional form misspecification. The idea behind RESET is quite simple: if we have properly specified the model, no nonlinear functions of the independant variables should be significant when added to our estimated equation.\n",
    "\n",
    "As the fitted, or predicted values, of the estimated model are linear in the independant values, we may consider powers of the predicted values as additional regressors. These values can not be added to the regression since they are by construction linear combinations of the X variable. But their squares/cubes are not. The RESET formulation reestimates the original equation, augmented by the square predicted value and conducts and F-test for the joint null hypothesis that those variables have no significant explanatory power.\n",
    "\n",
    "An alternative approach to RESET is the Davidson Mackinnon J Test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested Models\n",
    "\n",
    "Some models like Generlized linear models or structural equation models require that to compare model fit in competing models require that models be nested (specifically the likelihood ratio test).\n",
    "\n",
    "Model A is nested in Model B if the parameters of model A are a subset of the parameters in Model B. \n",
    "\n",
    "Let’s look at an example.\n",
    "\n",
    "We are predicting the Height of a shrub from the bacteria in the soil, which is measured continuously, and by the dummy-coded variable Sun, which has a value of 1 for a location in full sun and a value=0 for a location in partial sun.\n",
    "\n",
    "Model A:  Heighti = β0 + β1*Bacteria + β2*Sun + β3*Bacteria*Sun + εi\n",
    "\n",
    "This model has five parameters: β0 , β1 , β2 , β3 are all obvious, but there is one more, usually stated as an afterthought:  σ^2\n",
    "\n",
    "σ2 is the variance of the errors, εi. Sometimes you’ll see this written after the model, to make sure that this parameter and model assumptions are directly stated:\n",
    "\n",
    "εi ~ i.i.d. N(0, σ2)\n",
    "\n",
    "In your output you’ll see estimates of all 5 parameters. The four regression coefficients will usually be in one table and the residual variance is often in another. But they’re all there.\n",
    "\n",
    "Okay, so let’s compare this model to one in which we add a few covariates:\n",
    "\n",
    "Model B:  Heighti = β0 + β1*Bacteria + β2*Sun + β3*Bacteria*Sun + β4*Soil Nitrogen level + β5*Plant density + εi\n",
    "\n",
    "However, let’s consider a third model. Say we realized there was no real interaction between Soil Bacteria and Sun so we remove it to get Model C.\n",
    "\n",
    "Model C:  Heighti = β0 + β1*Bacteria + β2*Sun + β4*Soil Nitrogen level + β5*Plant density + εi\n",
    "\n",
    "Model A had 5 parameters:  β0, β1, β2, β3, and σ2\n",
    "\n",
    "Model B has 7:  β0, β1, β2, β3, β4, β5, and σ2\n",
    "\n",
    "Model C has 6:  β0, β1, β2, β4, β5, and σ2\n",
    "\n",
    "So Model A is nested in Model B.\n",
    "\n",
    "Model C is nested in Model B.\n",
    "\n",
    "But C and A are not nested. Each one contains parameters that the other doesn’t.\n",
    "\n",
    "I’ve shown this example with fixed effects parameters — the regression coefficients, but it works the same way when we compare models with different variance or covariance parameters, as occurs when we add random or repeated effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "Maximum likelihood estimation is a method that determines values for parameters of a model. The parameter values are found such that they maximize the likelihood that the process described by the model produced the data that were actually observed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
